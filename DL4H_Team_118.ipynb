{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **CS 598 Final Project Draft Team 118**\n",
        "\n",
        "Srikur Kanuparthy srikurk2@illinois.edu\n",
        "Kush Gupta kushg2@illinois.edu\n",
        "Justin Quall jquall2@illinois.edu\n",
        "\n",
        "GitHub Repo: https://github.com/justinqquall/CS_598_FINAL_PROJECT\n",
        "\n",
        "Project Video: https://mediaspace.illinois.edu/media/t/1_tdinfc0b "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "Citation to Original Paper: Alex, Labach., Aslesha, Pokhrel., Xiaolei, Huang., S., Zuberi., Seung, Eun, Yi., Maksims, Volkovs., Tomi, Poutanen., Rahul, G., Krishnan. (2023). DuETT: Dual Event Time Transformer for Electronic Health Records. arXiv.org, abs/2304.13017 doi: 10.48550/arXiv.2304.13017"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQ0sNuMePBXx"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "***DuETT: Dual Event Time Transformer for Electronic Health Records***\n",
        "\n",
        "The original paper presents a transformer-based deep learning architecture called DuETT (Dual\n",
        "Event Time Transformer for Electronic Health Records) which considers both time series and\n",
        "event type data. This is optimized for working with tabular data sets marked by sparsity and\n",
        "by irregular distribution over time, notably electronic health record (EHR) data.\n",
        "\n",
        "The main contribution is the novel architecture itself, which outperforms transformers\n",
        "considering time series and event modalities independently, as well as state-of-the-art deep\n",
        "learning frameworks like XGBoost, mTAND, and Raindrop. The models were all compared in a\n",
        "number of analyses on MIMIC-IV and PhysioNet-2012 datasets.\n",
        "Furthermore, other contributions are made to enable the DuETT architecture.\n",
        "\n",
        "These include\n",
        "designing an input representation which factors in event frequency and missingness, early\n",
        "fusion of static variables, and aggregates observations to allow deeper model structures to be\n",
        "used (without dramatically increasing computational needs).\n",
        "\n",
        "Additionally, the authors developed a new self-supervised training method which executes\n",
        "masking of measured event values and missingness for both time and event modalities.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uygL9tTPSVHB"
      },
      "source": [
        "# Scope of Reproducibility:\n",
        "\n",
        "Where feasible, we intend to reproduce the DuETT model and hypothesize seeing similar results to the authors original work.  The code is available in a decently well-documented public repository, so the biggest challenge will be fairly heavy computation demands.  Even with their thoughtful pre-processing techniques, the authors noted needed 2 days of running an Nvidia A6000 GPU for some DuETT pre-training and fine-turning steps.\n",
        "\n",
        "Due to that computational feasibility and data wrangling limitations, we do not expect to be able to run DuETT on both datasets (MIMIC-IV and PhysioNet-2012) within the scope of the project’s timelines.  Hence we will limit the scope of reproducibility to the January 20, 2012 version of PhysioNet-2012 (1.0.0).  \n",
        "\n",
        "HYPOTHESIS 1: We expect to be able to closely reproduce the core results on the PhysioNet-2012 dataset.  We will compare our results to those shown in Table 1 for the PhysioNet-2012 dataset, keeping the same key measures, ROC-AUC and PR-AUC.\n",
        "\n",
        "HYPOTHESIS 2: Furthermore, we plan to test a data ablation piece where 10-20% of the data is omitted for each patient sample.  This form of ablation may make sense here because of the paper’s use of novel self-supervised learning to augment model training in the context of limited labeled data.  This may be relevant to real-world healthcare practice, where EMRs may only cover the scope of one health system (whilst high-acuity patients receive care at multiple health systems on different EMRs/instances).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWAHJ_1CdtaA"
      },
      "source": [
        "# Methodology\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzVUQS0CHry0"
      },
      "source": [
        "First, we clone the GitHub repository for the paper and install its dependent libraries. Note that for the dependencies to work properly, we need to be using either Python version 3.8 or 3.9.  To successfully run the original model. Overall, the key software steps for overall replication of the core results were:\n",
        "\n",
        "1. Cloning the full public Github repository\n",
        "2. Setting up a virtual runtime environment running Python 3.9 (not specified below but possible using pyenv or venv)\n",
        "3. Installing the specific dependencies down to the version level as specified in the requirements.txt file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check that python version is either 3.8 or 3.9\n",
        "import sys\n",
        "if sys.version_info < (3, 8):\n",
        "    sys.exit('Python 3.8 or later is required to run this program.')\n",
        "if sys.version_info >= (3, 10):\n",
        "    sys.exit('Python 3.9 or earlier is required to run this program.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZScZNbROw-N"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/layer6ai-labs/DuETT.git\n",
        "%cd DuETT/\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NbPHUTMbkD3"
      },
      "source": [
        "##  Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The data is from the PhysioNet2012 challenge. The data consist of records from 12,000 ICU stays by patients, all adults who were admitted for various reasons. The dataset is publicly available, and the torchtime Python package makes it easy to load and use the data. 70 percent of the data is used for training, and the remaining 30 percent are used as test and validation sets.  Most of the data importing and processing occurs in our repository's \"physionet.py\" file (as originally published)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'tt_data = PhysioNet2012(self.split_name, train_prop=0.7, val_prop=0.15, time=False, seed=0)'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''tt_data = PhysioNet2012(self.split_name, train_prop=0.7, val_prop=0.15, time=False, seed=0)'''\n",
        "# Here, split_name is the name of the split to use. It can be one of 'train', 'val', 'test'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model\n",
        "\n",
        "Here is a citation to the original paper: Alex, Labach., Aslesha, Pokhrel., Xiaolei, Huang., S., Zuberi., Seung, Eun, Yi., Maksims, Volkovs., Tomi, Poutanen., Rahul, G., Krishnan. (2023). DuETT: Dual Event Time Transformer for Electronic Health Records. arXiv.org, abs/2304.13017 doi: 10.48550/arXiv.2304.13017\n",
        "\n",
        "And the original paper's repo is located at the following Github repository: https://github.com/layer6ai-labs/DuETT\n",
        "\n",
        "From the paper, the structure of the model is a series of layers followed by classification or self-supervised learning heads. Each layer is made up of two Transformer sublayers. \"The first sublayer consists of multi-head attention over events followed by a feed-forward network operating along the event dimension, which can be collectively identified as an event transformer layer; the second sublayer consists of multi-head attention over time bins followed by a feed-forward network operating along the time dimension, the time transformer layer.\"  After those \"DuETT\" combination layers, the results are fed into self-supervised learning heads for classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is the model class code in full:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''class Model(pl.LightningModule):\n",
        "    def __init__(self, d_static_num, d_time_series_num, d_target, lr=3.e-4, weight_decay=1.e-1, glu=False,\n",
        "            scalenorm=True, n_hidden_mlp_embedding=1, d_hidden_mlp_embedding=64, d_embedding=24, d_feedforward=512,\n",
        "            max_len=48, n_transformer_head=2, n_duett_layers=2, d_hidden_tab_encoder=128, n_hidden_tab_encoder=1,\n",
        "            norm_first=True, fusion_method='masked_embed', n_hidden_head=1, d_hidden_head=64, aug_noise=0., aug_mask=0.,\n",
        "            pretrain=True, pretrain_masked_steps=1, pretrain_n_hidden=0, pretrain_d_hidden=64, pretrain_dropout=0.5,\n",
        "            pretrain_value=True, pretrain_presence=True, pretrain_presence_weight=0.2, predict_events=True,\n",
        "            transformer_dropout=0., pos_frac=None, freeze_encoder=False, seed=0, save_representation=None,\n",
        "            masked_transform_timesteps=32, **kwargs):\n",
        "        super().__init__()\n",
        "        self.lr = lr\n",
        "        self.weight_decay = weight_decay\n",
        "        self.d_time_series_num = d_time_series_num\n",
        "        self.d_target = d_target\n",
        "        self.d_embedding = d_embedding\n",
        "        self.max_len = max_len\n",
        "        self.pretrain = pretrain\n",
        "        self.pretrain_masked_steps = pretrain_masked_steps\n",
        "        self.pretrain_dropout = pretrain_dropout\n",
        "        self.freeze_encoder = freeze_encoder\n",
        "        self.set_pos_frac(pos_frac)\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "        self.aug_noise = aug_noise\n",
        "        self.aug_mask = aug_mask\n",
        "        self.fusion_method = fusion_method\n",
        "        self.pretrain_presence = pretrain_presence\n",
        "        self.pretrain_presence_weight = pretrain_presence_weight\n",
        "        self.predict_events = predict_events\n",
        "        self.masked_transform_timesteps = masked_transform_timesteps\n",
        "        self.pretrain_value = pretrain_value\n",
        "        self.save_representation = save_representation\n",
        "        self.register_buffer(\"MASKED_EMBEDDING_KEY\", torch.tensor(0)) # For multi-gpu training\n",
        "        self.register_buffer(\"REPRESENTATION_EMBEDDING_KEY\", torch.tensor(1))\n",
        "\n",
        "        # For any special timesteps, e.g., masked, static, [CLS], etc.\n",
        "        self.special_embeddings = nn.Embedding(8, d_embedding)\n",
        "        self.embedding_layers = nn.ModuleList([\n",
        "            simple_mlp(2, d_embedding, n_hidden_mlp_embedding, d_hidden_mlp_embedding, hidden_batch_norm=True)\n",
        "            for _ in range(d_time_series_num)])\n",
        "\n",
        "        self.n_obs_embedding = nn.Embedding(16, 1)\n",
        "\n",
        "        if d_feedforward is None:\n",
        "            d_feedforward = d_embedding * 4\n",
        "\n",
        "        et_dim = d_embedding*(masked_transform_timesteps+1)\n",
        "        tt_dim = d_embedding*(d_time_series_num+1)\n",
        "        self.event_transformers = nn.ModuleList([x_transformers.Encoder(dim=et_dim, depth=1,\n",
        "                heads=n_transformer_head, pre_norm=norm_first, use_scalenorm=scalenorm,\n",
        "                attn_dim_head=d_embedding//n_transformer_head, ff_glu=glu,\n",
        "                ff_mult=d_feedforward/et_dim, attn_dropout=transformer_dropout,\n",
        "                ff_dropout=transformer_dropout) for _ in range(n_duett_layers)])\n",
        "        self.full_event_embedding = nn.Embedding(d_time_series_num + 1, et_dim)\n",
        "        self.time_transformers = nn.ModuleList([x_transformers.Encoder(dim=tt_dim, depth=1,\n",
        "                heads=n_transformer_head, pre_norm=norm_first, use_scalenorm=scalenorm,\n",
        "                attn_dim_head=d_embedding//n_transformer_head, ff_glu=glu,\n",
        "                ff_mult=d_feedforward/tt_dim, attn_dropout=transformer_dropout,\n",
        "                ff_dropout=transformer_dropout) for _ in range(n_duett_layers)])\n",
        "        self.full_time_embedding =  self.cve(batch_norm=True, d_embedding=tt_dim)\n",
        "        self.full_rep_embedding = nn.Embedding(tt_dim, 1)\n",
        "\n",
        "        d_representation = d_embedding * (d_time_series_num + 1) # time_series + static\n",
        "        self.head = simple_mlp(d_representation, d_target, n_hidden_head, d_hidden_head,\n",
        "                hidden_batch_norm=True, final_activation=False, activation=nn.ReLU)\n",
        "        self.pretrain_value_proj = simple_mlp(d_representation, d_time_series_num,\n",
        "                pretrain_n_hidden, pretrain_d_hidden, hidden_batch_norm=True)\n",
        "        if self.pretrain_presence:\n",
        "            self.pretrain_presence_proj = simple_mlp(d_representation, d_time_series_num,\n",
        "                    pretrain_n_hidden, pretrain_d_hidden, hidden_batch_norm=True)\n",
        "        if self.predict_events:\n",
        "            self.predict_events_proj = simple_mlp(et_dim, masked_transform_timesteps,\n",
        "                    pretrain_n_hidden, pretrain_d_hidden, hidden_batch_norm=True)\n",
        "            if self.pretrain_presence:\n",
        "                self.predict_events_presence_proj = simple_mlp(et_dim, masked_transform_timesteps,\n",
        "                        pretrain_n_hidden, pretrain_d_hidden, hidden_batch_norm=True)\n",
        "\n",
        "        self.tab_encoder = simple_mlp(d_static_num, d_embedding, n_hidden_tab_encoder,\n",
        "                    d_hidden_tab_encoder, hidden_batch_norm=True)\n",
        "\n",
        "        self.pretrain_loss = F.mse_loss\n",
        "        self.loss_function = F.binary_cross_entropy_with_logits\n",
        "        self.pretrain_presence_loss = F.binary_cross_entropy_with_logits\n",
        "        num_classes = None if d_target == 1 else d_target\n",
        "        self.train_auroc = torchmetrics.AUROC(num_classes=num_classes)\n",
        "        self.val_auroc = torchmetrics.AUROC(num_classes=num_classes)\n",
        "        self.train_ap = torchmetrics.AveragePrecision(num_classes=num_classes)\n",
        "        self.val_ap = torchmetrics.AveragePrecision(num_classes=num_classes)\n",
        "        self.test_auroc = torchmetrics.AUROC(num_classes=num_classes)\n",
        "        self.test_ap = torchmetrics.AveragePrecision(num_classes=num_classes)\n",
        "\n",
        "    def set_pos_frac(self, pos_frac):\n",
        "        if type(pos_frac) == list:\n",
        "            pos_frac = torch.tensor(pos_frac, device=torch.device('cuda'))\n",
        "        self.pos_frac = pos_frac\n",
        "        if pos_frac != None:\n",
        "            self.pos_weight = 1 / (2 * pos_frac)\n",
        "            self.neg_weight = 1 / (2 * (1 - pos_frac))\n",
        "\n",
        "    def cve(self, d_embedding=None, batch_norm=False):\n",
        "        if d_embedding == None:\n",
        "            d_embedding = self.d_embedding\n",
        "        d_hidden = int(np.sqrt(d_embedding))\n",
        "        if batch_norm:\n",
        "            return nn.Sequential(nn.Linear(1, d_hidden), nn.Tanh(), BatchNormLastDim(d_hidden), nn.Linear(d_hidden, d_embedding))\n",
        "        return nn.Sequential(nn.Linear(1, d_hidden), nn.Tanh(), nn.Linear(d_hidden, d_embedding))\n",
        "\n",
        "    def feats_to_input(self, x, batch_size, limits=None):\n",
        "        xs_ts, xs_static, times = x\n",
        "        xs_ts = list(xs_ts)\n",
        "\n",
        "        for i,f in enumerate(xs_ts):\n",
        "            n_vars = f.shape[1] // 2\n",
        "            if f.shape[0] > self.max_len:\n",
        "                f = f[-self.max_len:]\n",
        "                times[i] = times[i][-self.max_len:]\n",
        "            # Aug\n",
        "            if self.training and self.aug_noise > 0 and not self.pretrain:\n",
        "                f[:,:n_vars] += self.aug_noise * torch.randn_like(f[:,:n_vars]) * f[:,n_vars:]\n",
        "            f = torch.cat((f, torch.zeros_like(f[:,:1])), dim=1)\n",
        "            if self.training and self.aug_mask > 0 and not self.pretrain:\n",
        "                mask = torch.rand(f.shape[0]) < self.aug_mask\n",
        "                f[mask,:] = 0.\n",
        "                f[mask,-1] = 1.\n",
        "            xs_ts[i] = f\n",
        "        n_timesteps = [len(ts) for ts in times]\n",
        "\n",
        "        pad_to = np.max(n_timesteps)\n",
        "        xs_ts = torch.stack([F.pad(t, (0, 0, 0, pad_to-t.shape[0])) for t in xs_ts]).to(self.device)\n",
        "        xs_times = torch.stack([F.pad(t, (0, pad_to-t.shape[0])) for t in times]).to(self.device)\n",
        "        xs_static = torch.stack(xs_static).to(self.device)\n",
        "\n",
        "        if self.training and self.aug_noise > 0 and not self.pretrain:\n",
        "            xs_static += self.aug_noise * torch.randn_like(xs_static)\n",
        "\n",
        "        return xs_static, xs_ts, xs_times, n_timesteps\n",
        "\n",
        "    def pretrain_prep_batch(self, x, batch_size):\n",
        "        xs_static, xs_ts, xs_times, n_timesteps = self.feats_to_input(x, batch_size)\n",
        "        n_steps = xs_ts.shape[1]\n",
        "        n_vars = (xs_ts.shape[2] - 1) // 2\n",
        "        y_ts = []\n",
        "        y_ts_n_obs = []\n",
        "        y_events = []\n",
        "        y_events_mask = []\n",
        "        xs_ts_clipped = xs_ts.clone()\n",
        "        for batch_i, n in enumerate(n_timesteps):\n",
        "            if n < 2:\n",
        "                mask_i = n\n",
        "            elif self.pretrain_masked_steps > 1:\n",
        "                if self.pretrain_masked_steps > n:\n",
        "                    mask_i = np.arange(n)\n",
        "                else:\n",
        "                    mask_i = self.rng.choice(np.arange(n), size=self.pretrain_masked_steps)\n",
        "            else:\n",
        "                mask_i = self.rng.choice(np.arange(0, n))\n",
        "            y_ts.append(xs_ts[batch_i,mask_i,:n_vars])\n",
        "            y_ts_n_obs.append(xs_ts[batch_i,mask_i,n_vars:2*n_vars])\n",
        "\n",
        "            xs_ts_clipped[batch_i, mask_i, :] = 0.\n",
        "            xs_ts_clipped[batch_i,mask_i,-1] = 1.\n",
        "\n",
        "            if self.predict_events:\n",
        "                event_mask_i = self.rng.choice(np.arange(0, self.d_time_series_num))\n",
        "                y_events.append(xs_ts[batch_i, :, event_mask_i])\n",
        "                y_events_mask.append(xs_ts[batch_i, :, event_mask_i + n_vars].clip(0,1))\n",
        "                xs_ts_clipped[batch_i, :, event_mask_i] = 0\n",
        "                xs_ts_clipped[batch_i, :, event_mask_i + n_vars] = -1\n",
        "\n",
        "        y_ts = torch.stack(y_ts)\n",
        "        y_ts_n_obs = torch.stack(y_ts_n_obs)\n",
        "        y_ts_masks = y_ts_n_obs.clip(0,1)\n",
        "        if len(y_events) > 0:\n",
        "            y_events = torch.stack(y_events)\n",
        "            y_events_mask = torch.stack(y_events_mask)\n",
        "        if self.pretrain_dropout > 0:\n",
        "            keep = self.rng.random((batch_size, n_vars)) > self.pretrain_dropout\n",
        "            keep = torch.tensor(keep, device=xs_ts.device)\n",
        "            # Only drop out values that are unmasked in y\n",
        "            if y_ts_masks.ndim > 2:\n",
        "                keep = torch.logical_or(1 - y_ts_masks.sum(dim=1).clip(0,1), keep)\n",
        "            else:\n",
        "                keep = torch.logical_or(1 - y_ts_masks, keep)\n",
        "            keep = torch.cat((keep.tile(1,2), torch.ones((batch_size, 1), device=keep.device)), dim=1)\n",
        "            xs_ts_clipped *= torch.logical_or(keep.unsqueeze(1), xs_ts_clipped == -1)\n",
        "        return (xs_static, xs_ts_clipped, xs_times, n_timesteps), y_ts, y_ts_masks, y_events, y_events_mask\n",
        "\n",
        "    def forward(self, x, pretrain=False, representation=False):\n",
        "        \"\"\"\n",
        "        Forward run\n",
        "        :param x: input to the model\n",
        "        :return: prediction output (i.e., class probabilities vector)\n",
        "        \"\"\"\n",
        "        xs_static, xs_feats, xs_times, n_timesteps = x\n",
        "        n_vars = xs_feats.shape[2] // 2\n",
        "        if self.predict_events:\n",
        "            event_mask_inds = xs_feats[:,:,n_vars:n_vars*2] == -1\n",
        "            event_mask_inds = torch.cat((event_mask_inds, torch.zeros(xs_feats.shape[:2] + (1,), device=xs_feats.device, dtype=torch.bool)), dim=2)\n",
        "            event_mask_inds = torch.cat((event_mask_inds, event_mask_inds[:,:1,:]), dim=1)\n",
        "        n_obs_inds = xs_feats[:,:,n_vars:n_vars*2].to(int).clip(0, self.n_obs_embedding.num_embeddings - 1)\n",
        "        xs_feats[:,:,n_vars:n_vars*2] = self.n_obs_embedding(n_obs_inds).squeeze(-1)\n",
        "\n",
        "        embedding_layer_input = torch.empty(xs_feats.shape[:-1] + (n_vars, 2), dtype=xs_feats.dtype, device=xs_feats.device)\n",
        "        embedding_layer_input[:,:,:,0] = xs_feats[:,:,:n_vars]\n",
        "        embedding_layer_input[:,:,:,1] = xs_feats[:,:,n_vars:n_vars*2]\n",
        "        # dims: batch, time step, var, embedding\n",
        "        psi = torch.zeros((xs_feats.shape[0], xs_feats.shape[1]+1, n_vars+1, self.d_embedding), dtype=xs_feats.dtype, device=xs_feats.device)\n",
        "        for i, el in enumerate(self.embedding_layers):\n",
        "            psi[:,:-1,i,:] = el(embedding_layer_input[:,:,i,:])\n",
        "        psi[:,:-1,-1,:] = self.tab_encoder(xs_static).unsqueeze(1)\n",
        "        psi[:,-1,:,:] = self.special_embeddings(self.REPRESENTATION_EMBEDDING_KEY.to(self.device)).unsqueeze(0).unsqueeze(1)\n",
        "        mask_inds = torch.cat((xs_feats[:,:,-1] == 1, torch.zeros((xs_feats.shape[0], 1), device=xs_feats.device, dtype=torch.bool)), dim=1)\n",
        "        psi[mask_inds, :, :] = self.special_embeddings(self.MASKED_EMBEDDING_KEY.to(self.device))\n",
        "        if self.predict_events:\n",
        "            psi[event_mask_inds, :] = self.special_embeddings(self.MASKED_EMBEDDING_KEY.to(self.device))\n",
        "\n",
        "        # batch, time step, full embedding\n",
        "        time_embeddings = self.full_time_embedding(xs_times.unsqueeze(2))\n",
        "        time_embeddings = torch.cat((time_embeddings,\n",
        "            self.full_rep_embedding.weight.T.unsqueeze(0).expand(xs_feats.shape[0],-1,-1)),\n",
        "            dim=1)\n",
        "        for layer_i, (event_transformer, time_transformer) in enumerate(zip(self.event_transformers, self.time_transformers)):\n",
        "            et_out_shape = (psi.shape[0], psi.shape[2], psi.shape[1], psi.shape[3])\n",
        "            embeddings = psi.transpose(1,2).flatten(2) + self.full_event_embedding.weight.unsqueeze(0)\n",
        "            event_outs = event_transformer(embeddings).view(et_out_shape).transpose(1,2)\n",
        "            tt_out_shape = event_outs.shape\n",
        "            embeddings = event_outs.flatten(2) + time_embeddings\n",
        "            psi = time_transformer(embeddings).view(tt_out_shape)\n",
        "        transformed = psi.flatten(2)\n",
        "\n",
        "        if self.fusion_method == 'rep_token':\n",
        "            z_ts = transformed[:,-1,:]\n",
        "        elif self.fusion_method == 'masked_embed':\n",
        "            if self.pretrain_masked_steps > 1:\n",
        "                masked_ind = F.pad(xs_feats[:,:,-1] > 0, (0,1), value=False)\n",
        "                z_ts = []\n",
        "                for i in range(transformed.shape[0]):\n",
        "                    z_ts.append(F.pad(transformed[i, masked_ind[i],:], (0,0,0,self.pretrain_masked_steps-masked_ind[i].sum()), value=0.))\n",
        "                z_ts = torch.stack(z_ts) # batch size x pretrain_masked_steps x d_embedding\n",
        "            else:\n",
        "                masked_ind = xs_feats[:,:,-1:]\n",
        "                z_ts = []\n",
        "                for i in range(transformed.shape[0]):\n",
        "                    z_ts.append(transformed[i, torch.nonzero(masked_ind[i].squeeze()==1),:])\n",
        "                z_ts = torch.cat(z_ts, dim=0).squeeze()\n",
        "        elif self.fusion_method == 'averaging':\n",
        "            z_ts = torch.mean(transformed[:,:-1,:], dim=1)\n",
        "\n",
        "        z = z_ts\n",
        "        if representation:\n",
        "            return z\n",
        "\n",
        "        if pretrain:\n",
        "            rep_token_head = torch.tile(transformed[:,0,:].unsqueeze(1), (1, self.masked_transform_timesteps, 1))\n",
        "            y_hat_presence = self.pretrain_presence_proj(z).squeeze() if self.pretrain_presence else None\n",
        "            y_hat_value = self.pretrain_value_proj(z).squeeze(1) if self.pretrain_value else None\n",
        "            z_events = []\n",
        "            y_hat_events, y_hat_events_presence = None, None\n",
        "            if self.predict_events:\n",
        "                for i in range(event_mask_inds.shape[0]):\n",
        "                    z_events.append(psi[i][event_mask_inds[i].nonzero(as_tuple=True)].flatten())\n",
        "                z_events = torch.stack(z_events)\n",
        "                y_hat_events = self.predict_events_proj(z_events).squeeze()\n",
        "                y_hat_events_presence = self.predict_events_presence_proj(z_events).squeeze() if self.pretrain_presence else None\n",
        "            return y_hat_value, y_hat_presence, y_hat_events, y_hat_events_presence\n",
        "\n",
        "        out = self.head(z).squeeze(1)\n",
        "\n",
        "        if self.save_representation:\n",
        "            return out, z\n",
        "        else:\n",
        "            return out\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizers = [torch.optim.AdamW([p for l in self.modules() for p in l.parameters()],\n",
        "                lr=self.lr, weight_decay=self.weight_decay)]\n",
        "        return optimizers\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y = torch.tensor(y, dtype=torch.float64, device=self.device)\n",
        "        batch_size = y.shape[0]\n",
        "        if self.pretrain:\n",
        "            x_pretrain, y, mask, y_events, y_events_mask = self.pretrain_prep_batch(x, batch_size)\n",
        "            y_hat_value, y_hat_presence, y_hat_events, y_hat_events_presence = self.forward(x_pretrain, pretrain=True)\n",
        "\n",
        "            loss = 0\n",
        "            if self.pretrain_value:\n",
        "                if self.pretrain_masked_steps > 1:\n",
        "                    for i in range(self.pretrain_masked_steps):\n",
        "                        loss += self.pretrain_loss(y_hat_value[:,i]*mask[:,i], y[:,i]*mask[:,i])\n",
        "                    loss /= self.pretrain_masked_steps\n",
        "                else:\n",
        "                    loss = self.pretrain_loss(y_hat_value*mask, y*mask)\n",
        "            if self.pretrain_presence:\n",
        "                if self.pretrain_masked_steps > 1:\n",
        "                    presence_loss = 0\n",
        "                    for i in range(self.pretrain_masked_steps):\n",
        "                        presence_loss += self.pretrain_presence_loss(y_hat_presence[:,i], mask[:,i]) * self.pretrain_presence_weight\n",
        "                    presence_loss /= self.pretrain_masked_steps\n",
        "                else:\n",
        "                    presence_loss = self.pretrain_presence_loss(y_hat_presence, mask) * self.pretrain_presence_weight\n",
        "                loss += presence_loss\n",
        "            if self.predict_events:\n",
        "                if self.pretrain_value:\n",
        "                    loss += self.pretrain_loss(y_hat_events*y_events_mask, y_events*y_events_mask)\n",
        "                if self.pretrain_presence:\n",
        "                    loss += self.pretrain_presence_loss(y_hat_events_presence, y_events_mask) * self.pretrain_presence_weight\n",
        "        else:\n",
        "            y_hat = self.forward(self.feats_to_input(x, batch_size))\n",
        "            if self.pos_frac is not None:\n",
        "                weight = torch.where(y > 0, self.pos_weight, self.neg_weight)\n",
        "                loss = self.loss_function(y_hat, y, weight)\n",
        "            else:\n",
        "                loss = self.loss_function(y_hat, y)\n",
        "            self.train_auroc.update(y_hat, y.to(int))\n",
        "            self.train_ap.update(y_hat, y.to(int))\n",
        "\n",
        "        # Workaround to fix the loss=nan issue on the train progress bar\n",
        "        # self.trainer.train_loop.running_loss.append(loss)\n",
        "        self.log('train_loss', loss, sync_dist=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y = torch.tensor(y, dtype=torch.float64, device=self.device)\n",
        "        batch_size = y.shape[0]\n",
        "        if self.pretrain:\n",
        "            x_pretrain, y, mask, y_events, y_events_mask = self.pretrain_prep_batch(x, batch_size)\n",
        "            y_hat_value, y_hat_presence, y_hat_events, y_hat_events_presence = self.forward(x_pretrain, pretrain=True)\n",
        "\n",
        "            loss = 0\n",
        "            if self.pretrain_value:\n",
        "                if self.pretrain_masked_steps > 1:\n",
        "                    for i in range(self.pretrain_masked_steps):\n",
        "                        loss += self.pretrain_loss(y_hat_value[:,i]*mask[:,i], y[:,i]*mask[:,i])\n",
        "                    loss /= self.pretrain_masked_steps\n",
        "                else:\n",
        "                    loss = self.pretrain_loss(y_hat_value*mask, y*mask)\n",
        "                self.log('val_next_loss', loss, on_epoch=True, sync_dist=True, rank_zero_only=True)\n",
        "            if self.pretrain_presence:\n",
        "                if self.pretrain_masked_steps > 1:\n",
        "                    presence_loss = 0\n",
        "                    for i in range(self.pretrain_masked_steps):\n",
        "                        presence_loss += self.pretrain_presence_loss(y_hat_presence[:,i], mask[:,i]) * self.pretrain_presence_weight\n",
        "                    presence_loss /= self.pretrain_masked_steps\n",
        "                else:\n",
        "                    presence_loss = self.pretrain_presence_loss(y_hat_presence, mask) * self.pretrain_presence_weight\n",
        "                self.log('val_presence_loss', presence_loss, on_epoch=True, sync_dist=True, rank_zero_only=True)\n",
        "                loss += presence_loss\n",
        "            if self.predict_events:\n",
        "                event_loss = self.pretrain_loss(y_hat_events*y_events_mask, y_events*y_events_mask)\n",
        "                self.log('val_event_loss', event_loss, on_epoch=True, sync_dist=True, rank_zero_only=True)\n",
        "                loss += event_loss\n",
        "        else:\n",
        "            y_hat = self.forward(self.feats_to_input(x, batch_size))\n",
        "            if self.pos_frac is not None:\n",
        "                weight = torch.where(y > 0, self.pos_weight, self.neg_weight)\n",
        "                loss = self.loss_function(y_hat, y, weight)\n",
        "            else:\n",
        "                loss = self.loss_function(y_hat, y)\n",
        "            self.val_auroc.update(y_hat, y.to(int).to(self.device))\n",
        "            self.val_ap.update(y_hat, y.to(int).to(self.device))\n",
        "\n",
        "        if not self.pretrain:\n",
        "            self.log('val_ap', self.val_ap, on_epoch=True, sync_dist=True, rank_zero_only=True)\n",
        "            self.log('val_auroc', self.val_auroc, on_epoch=True, sync_dist=True, rank_zero_only=True)\n",
        "        self.log('val_loss', loss, on_epoch=True, sync_dist=True, prog_bar=True, rank_zero_only=True)\n",
        "\n",
        "    def training_epoch_end(self, training_step_outputs):\n",
        "        if not self.pretrain:\n",
        "            self.log('train_auroc', self.train_auroc, sync_dist=True, rank_zero_only=True)\n",
        "            self.log('train_ap', self.train_ap, sync_dist=True, rank_zero_only=True)\n",
        "\n",
        "    def validation_epoch_end(self, validation_step_outputs):\n",
        "        if not self.pretrain:\n",
        "            print(\"val_auroc\", self.val_auroc.compute(), \"val_ap\", self.val_ap.compute())\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y = torch.tensor(y, dtype=torch.float64, device=self.device)\n",
        "        batch_size = y.shape[0]\n",
        "        if self.save_representation:\n",
        "            y_hat, z = self.forward(self.feats_to_input(x, batch_size))\n",
        "\n",
        "            print(\"saving representations...\")\n",
        "            with open(self.save_representation, 'ab') as f:\n",
        "                if y.ndim == 1:\n",
        "                    np.savetxt(f,np.concatenate([z.cpu(), y.unsqueeze(1).cpu()], axis=1))\n",
        "                else:\n",
        "                    np.savetxt(f,np.concatenate([z.cpu(), y.cpu()], axis=1))\n",
        "        else:\n",
        "            y_hat = self.forward(self.feats_to_input(x, batch_size))\n",
        "        if self.pos_frac is not None:\n",
        "            weight = torch.where(y > 0, self.pos_weight, self.neg_weight)\n",
        "            loss = self.loss_function(y_hat, y, weight)\n",
        "        else:\n",
        "            loss = self.loss_function(y_hat, y)\n",
        "        self.log('test_loss', loss, on_epoch=True, sync_dist=True, rank_zero_only=True)\n",
        "        self.test_auroc.update(y_hat, y.to(int).to(self.device))\n",
        "        self.log('test_auroc', self.test_auroc, on_epoch=True, sync_dist=True, rank_zero_only=True)\n",
        "        self.test_ap.update(y_hat, y.to(int).to(self.device))\n",
        "        self.log('test_ap', self.test_ap, on_epoch=True, sync_dist=True, rank_zero_only=True)\n",
        "\n",
        "        return loss, self.test_auroc, self.test_ap\n",
        "\n",
        "    def on_load_checkpoint(self, checkpoint):\n",
        "        # Ignore errors from size mismatches in head, since those might change between pretraining\n",
        "        # and supervised training\n",
        "        # Adapted from https://github.com/PyTorchLightning/pytorch-lightning/issues/4690#issuecomment-731152036\n",
        "        print('Loading from checkpoint')\n",
        "        state_dict = checkpoint[\"state_dict\"]\n",
        "        model_state_dict = self.state_dict()\n",
        "        is_changed = False\n",
        "        for k in model_state_dict:\n",
        "            if k not in state_dict:\n",
        "                state_dict[k] = model_state_dict[k]\n",
        "                is_changed = True\n",
        "        for k in state_dict:\n",
        "            if k in model_state_dict:\n",
        "                if k.startswith('head') and state_dict[k].shape != model_state_dict[k].shape:\n",
        "                    print(f\"Skip loading parameter: {k}, \"\n",
        "                                f\"required shape: {model_state_dict[k].shape}, \"\n",
        "                                f\"loaded shape: {state_dict[k].shape}\")\n",
        "                    state_dict[k] = model_state_dict[k]\n",
        "                    is_changed = True\n",
        "            else:\n",
        "                print(f\"Dropping parameter {k}\")\n",
        "                is_changed = True\n",
        "\n",
        "        if is_changed:\n",
        "            checkpoint.pop(\"optimizer_states\", None)\n",
        "\n",
        "        if self.freeze_encoder:\n",
        "            self.freeze()\n",
        "\n",
        "    def freeze(self):\n",
        "        print('Freezing')\n",
        "        for n, w in self.named_parameters():\n",
        "            if \"head\" not in n:\n",
        "                w.requires_grad = False\n",
        "            else:\n",
        "                print(\"Skip freezing:\", n)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3hs3ZryGaPc"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Explanation of selected hyperparameters used in model training: \n",
        "1. Maximum Epochs (max_epochs): This parameter is set for the trainer and defines the maximum number of epochs to run the training process. It is set to 300 in the pretraining phase and 50 in the fine-tuning phase.\n",
        "2. Gradient Clipping Value (gradient_clip_val): Set in the trainer to a value of 1.0. Gradient clipping is a technique used to prevent exploding gradients in neural networks by capping them to a maximum value during backpropagation.\n",
        "3. Learning Rate Decay (decay): This is used in the WarmUpCallback to adjust the rate of learning rate decay after the warm-up period. The default is set to the same as steps if not specified.\n",
        "\n",
        "The **WarmUpCallback** class is designed to manage learning rate adjustments during model training. It starts with a warm-up period where the learning rate increases linearly from zero to a predefined or automatically detected base learning rate over a specified number of steps. Once the warm-up phase is completed, the callback can optionally apply an inverse square root decay to the learning rate for additional specified steps. This approach helps in stabilizing the model's training early on by gradually increasing the learning rate and then adjusting it to prevent overshooting as training progresses. The callback also includes functionalities to save and load its state, facilitating training resumption or state transfer between sessions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code below loads a batch of the physionet data and pretrains the model using the duett model. This pretrained model is then Checkpointed and saved. The pl Trainer then fits the pretrained model on the physionet data that we pulled. Since the training takes a while, this step is commented out and we can use the checkpointed model to run the next steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\npl.seed_everything(seed)\\ndm = physionet.PhysioNetDataModule(batch_size=512, num_workers=16, use_temp_cache=True)\\ndm.setup()\\npretrain_model = duett.pretrain_model(d_static_num=dm.d_static_num(),\\n        d_time_series_num=dm.d_time_series_num(), d_target=dm.d_target(), pos_frac=dm.pos_frac(),\\n        seed=seed)\\ncheckpoint = pl.callbacks.ModelCheckpoint(save_last=True, monitor='val_loss', mode='min', save_top_k=1, dirpath='checkpoints')\\nwarmup = WarmUpCallback(steps=2000)\\ntrainer = pl.Trainer(gpus=1, logger=False, num_sanity_val_steps=2, max_epochs=300,\\n        gradient_clip_val=1.0, callbacks=[warmup, checkpoint])\\ntrainer.fit(pretrain_model, dm)\\n\""
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "pl.seed_everything(seed)\n",
        "dm = physionet.PhysioNetDataModule(batch_size=512, num_workers=16, use_temp_cache=True)\n",
        "dm.setup()\n",
        "pretrain_model = duett.pretrain_model(d_static_num=dm.d_static_num(),\n",
        "        d_time_series_num=dm.d_time_series_num(), d_target=dm.d_target(), pos_frac=dm.pos_frac(),\n",
        "        seed=seed)\n",
        "checkpoint = pl.callbacks.ModelCheckpoint(save_last=True, monitor='val_loss', mode='min', save_top_k=1, dirpath='checkpoints')\n",
        "warmup = WarmUpCallback(steps=2000)\n",
        "trainer = pl.Trainer(gpus=1, logger=False, num_sanity_val_steps=2, max_epochs=300,\n",
        "        gradient_clip_val=1.0, callbacks=[warmup, checkpoint])\n",
        "trainer.fit(pretrain_model, dm)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The computational requirements were fairly vast. To get the model trained in less than three hours, we needed to use a virtual machine with 32 GB of GPU memory, basically fully dedicated to the task. Compared to local hardware acceleration on an upgraded commercial Macbook, this reduce runtime per epoch from around 5 minutes per epoch to around 30 seconds per epoch (10x faster). With the standard training completed over 300 epochs, this all added up pretty quickly and turned out to be a very computationally intensive model to train."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gX6bCcZNuxmz"
      },
      "source": [
        "## Results\n",
        "\n",
        "*** Note: due to computational intensity, the model was trained and fine-tuned on a virtual machine with strong hardware acceleration (via T4 GPU). ***\n",
        "\n",
        "The commented out code blocks below will load in the dataset and run through different key steps of setting up the model parameters and training functions.  Please uncomment these out if you'd like to go step-by-step, otherwise at the end of this section the full results from running on a GPU-accelerated virtual machine are shown (this is what was done to reproduce the paper's metrics as closely as possible)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "79nwsLydGfUp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validating cache...\n",
            "Validating cache...\n",
            "Validating cache...\n"
          ]
        }
      ],
      "source": [
        "# import torch\n",
        "# from DuETT import duett\n",
        "# from DuETT import physionet\n",
        "\n",
        "# # Load the PhysioNet2012 Data\n",
        "# dm = physionet.PhysioNetDataModule(batch_size=512, num_workers=16)\n",
        "# dm.setup()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following class and average_models function are copied directly from train.py in the repository, since the file contains top level code that we do not wish to run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import pytorch_lightning as pl\n",
        "# from pathlib import Path\n",
        "\n",
        "# class WarmUpCallback(pl.callbacks.Callback):\n",
        "#     \"\"\"Linear warmup over warmup_steps batches, tries to auto-detect the base lr\"\"\"\n",
        "#     def __init__(self, steps=1000, base_lr=None, invsqrt=True, decay=None):\n",
        "#         print('warmup_steps {}, base_lr {}, invsqrt {}, decay {}'.format(steps, base_lr, invsqrt, decay))\n",
        "#         self.warmup_steps = steps\n",
        "#         if decay is None:\n",
        "#             self.decay = steps\n",
        "#         else:\n",
        "#             self.decay = decay\n",
        "\n",
        "#         if base_lr is None:\n",
        "#             self.state = {'steps': 0, 'base_lr': base_lr}\n",
        "#         else:\n",
        "#             self.state = {'steps': 0, 'base_lr': float(base_lr)}\n",
        "\n",
        "#         self.invsqrt = invsqrt\n",
        "\n",
        "#     def set_lr(self, optimizer, lr):\n",
        "#         for param_group in optimizer.param_groups:\n",
        "#             param_group['lr'] = lr\n",
        "\n",
        "#     def on_train_batch_start(self, trainer, model, batch, batch_idx):\n",
        "#         optimizers = model.optimizers()\n",
        "\n",
        "#         if self.state['steps'] < self.warmup_steps:\n",
        "#             if type(optimizers) == 'list':\n",
        "#                 if self.state['base_lr'] is None:\n",
        "#                     self.state['base_lr'] = [o.param_groups[0]['lr'] for o in optimizers]\n",
        "#                 for opt,base in zip(optimizers, self.state['base_lr']):\n",
        "#                     self.set_lr(opt, self.state['steps']/self.warmup_steps * base)\n",
        "#             else:\n",
        "#                 if self.state['base_lr'] is None:\n",
        "#                     self.state['base_lr'] = optimizers.param_groups[0]['lr']\n",
        "#                 self.set_lr(optimizers, self.state['steps']/self.warmup_steps * self.state['base_lr'])\n",
        "#             self.state['steps'] += 1\n",
        "#         elif self.invsqrt:\n",
        "#             if type(optimizers) == 'list':\n",
        "#                 if self.state['base_lr'] is None:\n",
        "#                     self.state['base_lr'] = [o.param_groups[0]['lr'] for o in optimizers]\n",
        "#                 for opt,base in zip(optimizers, self.state['base_lr']):\n",
        "#                     self.set_lr(opt,base * (self.decay / (self.state['steps'] - self.warmup_steps + self.decay)) ** 0.5)\n",
        "#             else:\n",
        "#                 if self.state['base_lr'] is None:\n",
        "#                     self.state['base_lr'] = optimizers.param_groups[0]['lr']\n",
        "#                 self.set_lr(optimizers, self.state['base_lr'] * (\n",
        "#                             self.decay / (self.state['steps'] - self.warmup_steps + self.decay)) ** 0.5)\n",
        "#             self.state['steps'] += 1\n",
        "\n",
        "#     def load_state_dict(self, state_dict):\n",
        "#         self.state.update(state_dict)\n",
        "\n",
        "#     def state_dict(self):\n",
        "#         return self.state.copy()\n",
        "\n",
        "# def average_models(models):\n",
        "#     \"\"\"Averages model weights and loads the resulting weights into the first model, returning it\"\"\"\n",
        "#     models = list(models)\n",
        "#     n = len(models)\n",
        "#     sds = [m.state_dict() for m in models]\n",
        "#     averaged = {}\n",
        "#     for k in sds[0]:\n",
        "#         averaged[k] = sum(sd[k] for sd in sds) / n\n",
        "#     models[0].load_state_dict(averaged)\n",
        "#     return models[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we can load the pre-trained model from checkpoint (also included in this repository) and fine-tune the model and test it using the validation set.\n",
        "\n",
        "Please note that here we only use 1 epoch for demonstration purposes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6BzSPKcXaZi8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Global seed set to 1234\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validating cache...\n",
            "Validating cache...\n",
            "Validating cache...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Global seed set to 2020\n",
            "/Users/srikur/miniconda3/envs/cs598env/lib/python3.9/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `AUROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "/Users/srikur/miniconda3/envs/cs598env/lib/python3.9/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `AveragePrecision` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading from checkpoint\n",
            "warmup_steps 1000, base_lr None, invsqrt True, decay None\n",
            "Validating cache...\n",
            "Validating cache...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/srikur/miniconda3/envs/cs598env/lib/python3.9/site-packages/torch/optim/adamw.py:92: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
            "  super(AdamW, self).__init__(params, defaults)\n",
            "\n",
            "   | Name                         | Type             | Params\n",
            "-------------------------------------------------------------------\n",
            "0  | special_embeddings           | Embedding        | 192   \n",
            "1  | embedding_layers             | ModuleList       | 67.7 K\n",
            "2  | n_obs_embedding              | Embedding        | 16    \n",
            "3  | event_transformers           | ModuleList       | 1.8 M \n",
            "4  | full_event_embedding         | Embedding        | 29.3 K\n",
            "5  | time_transformers            | ModuleList       | 2.0 M \n",
            "6  | full_time_embedding          | Sequential       | 26.8 K\n",
            "7  | full_rep_embedding           | Embedding        | 888   \n",
            "8  | head                         | Sequential       | 57.1 K\n",
            "9  | pretrain_value_proj          | Sequential       | 32.0 K\n",
            "10 | pretrain_presence_proj       | Sequential       | 32.0 K\n",
            "11 | predict_events_proj          | Sequential       | 25.4 K\n",
            "12 | predict_events_presence_proj | Sequential       | 25.4 K\n",
            "13 | tab_encoder                  | Sequential       | 4.5 K \n",
            "14 | train_auroc                  | AUROC            | 0     \n",
            "15 | val_auroc                    | AUROC            | 0     \n",
            "16 | train_ap                     | AveragePrecision | 0     \n",
            "17 | val_ap                       | AveragePrecision | 0     \n",
            "18 | test_auroc                   | AUROC            | 0     \n",
            "19 | test_ap                      | AveragePrecision | 0     \n",
            "-------------------------------------------------------------------\n",
            "4.1 M     Trainable params\n",
            "0         Non-trainable params\n",
            "4.1 M     Total params\n",
            "16.279    Total estimated model params size (MB)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0a1bc2925fc941fea150aff65db0b920",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Sanity Checking: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_auroc tensor(0.4938) val_ap tensor(0.1595)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "91baf16743624471968602fc9c4e54c4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f9a3e1b4aa46408280dff87cc6c2dc6b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_auroc tensor(0.5331) val_ap tensor(0.1652)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/srikur/miniconda3/envs/cs598env/lib/python3.9/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `AUROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "/Users/srikur/miniconda3/envs/cs598env/lib/python3.9/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `AveragePrecision` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "Global seed set to 2021\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading from checkpoint\n",
            "Loading from checkpoint\n",
            "warmup_steps 1000, base_lr None, invsqrt True, decay None\n",
            "Validating cache...\n",
            "Validating cache...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/srikur/miniconda3/envs/cs598env/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:608: UserWarning: Checkpoint directory checkpoints exists and is not empty.\n",
            "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
            "/Users/srikur/miniconda3/envs/cs598env/lib/python3.9/site-packages/torch/optim/adamw.py:92: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
            "  super(AdamW, self).__init__(params, defaults)\n",
            "\n",
            "   | Name                         | Type             | Params\n",
            "-------------------------------------------------------------------\n",
            "0  | special_embeddings           | Embedding        | 192   \n",
            "1  | embedding_layers             | ModuleList       | 67.7 K\n",
            "2  | n_obs_embedding              | Embedding        | 16    \n",
            "3  | event_transformers           | ModuleList       | 1.8 M \n",
            "4  | full_event_embedding         | Embedding        | 29.3 K\n",
            "5  | time_transformers            | ModuleList       | 2.0 M \n",
            "6  | full_time_embedding          | Sequential       | 26.8 K\n",
            "7  | full_rep_embedding           | Embedding        | 888   \n",
            "8  | head                         | Sequential       | 57.1 K\n",
            "9  | pretrain_value_proj          | Sequential       | 32.0 K\n",
            "10 | pretrain_presence_proj       | Sequential       | 32.0 K\n",
            "11 | predict_events_proj          | Sequential       | 25.4 K\n",
            "12 | predict_events_presence_proj | Sequential       | 25.4 K\n",
            "13 | tab_encoder                  | Sequential       | 4.5 K \n",
            "14 | train_auroc                  | AUROC            | 0     \n",
            "15 | val_auroc                    | AUROC            | 0     \n",
            "16 | train_ap                     | AveragePrecision | 0     \n",
            "17 | val_ap                       | AveragePrecision | 0     \n",
            "18 | test_auroc                   | AUROC            | 0     \n",
            "19 | test_ap                      | AveragePrecision | 0     \n",
            "-------------------------------------------------------------------\n",
            "4.1 M     Trainable params\n",
            "0         Non-trainable params\n",
            "4.1 M     Total params\n",
            "16.279    Total estimated model params size (MB)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6fbe6c46b5d04951a392824966768575",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Sanity Checking: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_auroc tensor(0.4938) val_ap tensor(0.1595)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a5d2e934a2014b40a67f9f37f90bf875",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5a972696284a4a9ca12083193f667f1f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_auroc tensor(0.5325) val_ap tensor(0.1654)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/srikur/miniconda3/envs/cs598env/lib/python3.9/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `AUROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "/Users/srikur/miniconda3/envs/cs598env/lib/python3.9/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `AveragePrecision` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "Global seed set to 2022\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading from checkpoint\n",
            "Loading from checkpoint\n",
            "warmup_steps 1000, base_lr None, invsqrt True, decay None\n",
            "Validating cache...\n",
            "Validating cache...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/srikur/miniconda3/envs/cs598env/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:608: UserWarning: Checkpoint directory checkpoints exists and is not empty.\n",
            "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
            "/Users/srikur/miniconda3/envs/cs598env/lib/python3.9/site-packages/torch/optim/adamw.py:92: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
            "  super(AdamW, self).__init__(params, defaults)\n",
            "\n",
            "   | Name                         | Type             | Params\n",
            "-------------------------------------------------------------------\n",
            "0  | special_embeddings           | Embedding        | 192   \n",
            "1  | embedding_layers             | ModuleList       | 67.7 K\n",
            "2  | n_obs_embedding              | Embedding        | 16    \n",
            "3  | event_transformers           | ModuleList       | 1.8 M \n",
            "4  | full_event_embedding         | Embedding        | 29.3 K\n",
            "5  | time_transformers            | ModuleList       | 2.0 M \n",
            "6  | full_time_embedding          | Sequential       | 26.8 K\n",
            "7  | full_rep_embedding           | Embedding        | 888   \n",
            "8  | head                         | Sequential       | 57.1 K\n",
            "9  | pretrain_value_proj          | Sequential       | 32.0 K\n",
            "10 | pretrain_presence_proj       | Sequential       | 32.0 K\n",
            "11 | predict_events_proj          | Sequential       | 25.4 K\n",
            "12 | predict_events_presence_proj | Sequential       | 25.4 K\n",
            "13 | tab_encoder                  | Sequential       | 4.5 K \n",
            "14 | train_auroc                  | AUROC            | 0     \n",
            "15 | val_auroc                    | AUROC            | 0     \n",
            "16 | train_ap                     | AveragePrecision | 0     \n",
            "17 | val_ap                       | AveragePrecision | 0     \n",
            "18 | test_auroc                   | AUROC            | 0     \n",
            "19 | test_ap                      | AveragePrecision | 0     \n",
            "-------------------------------------------------------------------\n",
            "4.1 M     Trainable params\n",
            "0         Non-trainable params\n",
            "4.1 M     Total params\n",
            "16.279    Total estimated model params size (MB)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "de83f7992e5a4841adc2d2e32335f4b1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Sanity Checking: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_auroc tensor(0.4938) val_ap tensor(0.1595)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "febbbbce0e5d45eabc32b4dbbaee06f1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "92ed0ad938ae4bce83c3b43127089bc9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_auroc tensor(0.5303) val_ap tensor(0.1642)\n",
            "Loading from checkpoint\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/srikur/miniconda3/envs/cs598env/lib/python3.9/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `AUROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "/Users/srikur/miniconda3/envs/cs598env/lib/python3.9/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `AveragePrecision` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
            "  warnings.warn(*args, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "import pytorch_lightning as pl\n",
        "# multiprocessing.set_start_method(\"fork\", force=True)\n",
        "\n",
        "path = \"https://github.com/srikur/cs_598_final_project/raw/main/model.ckpt\"\n",
        "seed = 1234\n",
        "pl.seed_everything(seed)\n",
        "dm = physionet.PhysioNetDataModule(batch_size=512, num_workers=16, use_temp_cache=True)\n",
        "dm.setup()\n",
        "for seed in range(2020, 2023):\n",
        "    pl.seed_everything(seed)\n",
        "\n",
        "    # performs fine-tuning and then averages the models. final_model is the averaged model\n",
        "    fine_tune_model = duett.fine_tune_model(path, d_static_num=dm.d_static_num(),\n",
        "            d_time_series_num=dm.d_time_series_num(), d_target=dm.d_target(), pos_frac=dm.pos_frac(), seed=seed, max_epochs=1)\n",
        "    checkpoint = pl.callbacks.ModelCheckpoint(save_top_k=5, save_last=False, mode='max', monitor='val_ap', dirpath='checkpoints')\n",
        "    warmup = WarmUpCallback(steps=1000)\n",
        "    trainer = pl.Trainer(gpus=0, logger=False, max_epochs=1, gradient_clip_val=1.0,\n",
        "            callbacks=[warmup, checkpoint])\n",
        "    trainer.fit(fine_tune_model, dm)\n",
        "    final_model = average_models([duett.fine_tune_model(path, d_static_num=dm.d_static_num(),\n",
        "            d_time_series_num=dm.d_time_series_num(), d_target=dm.d_target(), pos_frac=dm.pos_frac())\n",
        "            for path in checkpoint.best_k_models.keys()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validating cache...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a20d9a52bd8a4d2883526b25c7e37e41",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Testing: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "       Test metric             DataLoader 0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "         test_ap            0.14620420336723328\n",
            "       test_auroc           0.4836210608482361\n",
            "        test_loss           0.7418224269111838\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'test_loss': 0.7418224269111838,\n",
              "  'test_auroc': 0.4836210608482361,\n",
              "  'test_ap': 0.14620420336723328}]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# # Make predictions on the validation set\n",
        "trainer.test(final_model, dataloaders=dm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To train the model and show it's effectiveness on a validation set of data, the following command can be run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !python3 train.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here are the results from running through the full 300-epoch training and 50-epoch finetuning steps of the original paper, and a discussion of the different hardware configurations used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running the original code on a virtual machine with native GPU acceleration (V100) took approximately 3 hours:\n",
        "\n",
        "![alt text](image-1.png)\n",
        "\n",
        "VM specs:\n",
        "\n",
        "![alt text](image-3.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running the original file on a local machine with MPS hardware acceleration took approximately 24 hours:\n",
        "\n",
        "![alt text](image-2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[ ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ablation Analysis\n",
        "\n",
        "We tested a data ablation piece where 20% of the data is omitted for each patient sample.  This form of ablation may make sense here because of the paper’s use of novel self-supervised learning to augment model training in the context of limited labeled data.  This may be relevant to real-world healthcare practice, where EHRs may only cover the scope of one health system (whilst high-acuity patients receive care at multiple health systems on different EHRs/instances).  The authors tested various other types of model feature ablations within the original work, for example a single vs. double transformer architecture, self-supervised learning, and input representation.\n",
        "\n",
        "Here is the code which can added to the original physionet.py to conduct the ablation training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "    #   def setup(self):\n",
        "    #     tt_data = PhysioNet2012(self.split_name, train_prop=0.7, val_prop=0.15, time=False, seed=0)\n",
        "\n",
        "    #     self.X = tt_data.X\n",
        "    #     self.y = tt_data.y\n",
        "\n",
        "    #     # ...\n",
        "\n",
        "    #     # Application of Sparsify ablation data preparation (next 3 lines of code)\n",
        "\n",
        "    #     for i in range(self.X.shape[0]):\n",
        "    #         if self.split_name == 'train':\n",
        "    #             self.sparsify(self.X[i])\n",
        "\n",
        "    #     # ...\n",
        "\n",
        "    #   # Application of Sparsify data prep\n",
        "\n",
        "    #   def sparsify(self, vals):\n",
        "    #       for idx in range(215):\n",
        "    #           if (idx + 1) % 5 == 0 :\n",
        "    #               vals[idx] = torch.full((45,), float('nan'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EAWAy_LwHlV"
      },
      "source": [
        "## Model comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![alt text](image.png)\n",
        "\n",
        "The paper's original results on the PhysioNet-2012 dataset included a ROC-AUC of 0.872 and a PR-AUC of 0.564. While DuETT model is super neat and thoughtful, subjectively, these results did not seem to be a meaningful improvement over XGBoost (0.865 and 0.531, respectively). With XGBoost's reputation for being a fast and robust model type for working with tabular data, it seems like DuETT may not be be worth the difference in cost and support at this time.\n",
        "\n",
        "In any case, we were able to reproduce the paper's results on the PhysioNet-2012 dataset very closely. The reproduced model showed a ROC-AUC of 0.871 (within 0.001 of the paper's 0.872) and a PR-AUC of 0.563 (within 0.001 of the paper's 0.563). These results are aligned with the hypothesis of being able to recreate the original paper's results pretty closely. Please find a chart comparing the model resutls below.\n",
        "\n",
        "Within the original paper, the authors ran an incredibly broad set of ablations, including ablating event- and time-components of the transformer (running on one type only), an ablation analysis of the self-supervised learning ablation, and an input representation ablation. All of these were run using the MIMIC-IV dataset.\n",
        "\n",
        "We extended the ablation analysis by also considering the impact of reducing the thoroughness of the training set - specifically, removing 20% of the actual observation time bins (every fifth one) for each patient. We run this analysis on the PhysioNet-2012 dataset and include all patients (testing for \"sparsity\" within each patient) as compared to the paper's data redution analysis which was on MIMIC-IV and may have excluded entire patient records (unclear).\n",
        "\n",
        "This \"sparsity\" ablation is a practically meaningful consideration given the model's goal of being effective on sparse datasets and given real-world healthcare applications, where one hospital's electronic health record will often miss significant data corresponding with patients being treated by other healthcare providers (who utilize other EHR instances).\n",
        "\n",
        "The ablation study's results were quite interesting. The reduction to PR-AUC and ROC-AU were modest, to 0.531 and 0.866 respectively. Those metrics exceed or are close to the \"thorough dataset\" results of other top deep learning models such as LSTM, mTAND, Raindrop, etc - and were on par with the industry standard (for tabular machine learning) XGBoost.\n",
        "\n",
        "Further confirmatory analysis would be needed, but this indicates that DuETT may indeed successfully solve for training and inference on sparse, irregular datasets as are observed in real-world EHR data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qH75TNU71eRH"
      },
      "source": [
        "# Discussion\n",
        "\n",
        "As shown in the Results section, the paper was confirmed to be reproducible when focusing on application to the PhysioNet-2012 dataset.  \n",
        "\n",
        "During the reproduction, we found that running the code locally was fairly easy once the proper environment was established (for example running the correct version of dependency packages), however maintaining an aligned environment within Google CoLab took some getting used to.  \n",
        "\n",
        "It was also pretty easy to learn the big picture goals and structure of the DuETT model, thanks to a clear and well-written paper by the original authors.\n",
        "\n",
        "During the reproduction, the three most difficult things to address were hardware limitations, specific dependencies, and developing a robust understanding of the model's input data presentation.\n",
        "\n",
        "Even when focusing on the PhysioNet-2012 dataset (a fraction of the size of MIMIC-IV), training the sizeable and complex DuETT model required significant hardware capacity.  With local GPU acceleration (via PyTorch MPS) on a M1 Silicon Macbook with upgraded memory, training the model took ~22 hours and it was not feasible to iterate and debug.  This was solved by renting a virtual machine with a very powerful GPU (Nvidia Tesla V100), which reduced the training time to ~2.5 hours.\n",
        "\n",
        "Specific dependencies also led to some initial headaches while setting up a proper runtime environment.  Because the paper is fairly recent, this was not expected.  But perhaps due to how fast the AI field is evolving, we found specific matching of the requirements.txt dependencies was key (as was downgrading the runtime version of Python to 3.9).\n",
        "\n",
        "Finally, as with most of the models we ran in DLH, it was important to keep in mind the data pre-processing steps and how the tensors were all set up (key thing here being the 8,400 x 215 x 45 dimensions of patients, potential visit time bins, and attributes; respectively).  Incorrect early assumptions on this drove a need for debugging issues such as dimension mix-ups and an off-by-one error.\n",
        "\n",
        "We do not have further suggestions to the authors on how to improve the reproducibility, as they did a great job of setting up a fairly well-documented public repository.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHMI2chl9omn"
      },
      "source": [
        "# References\n",
        "\n",
        "1. Alex, Labach., Aslesha, Pokhrel., Xiaolei, Huang., S., Zuberi., Seung, Eun, Yi., Maksims, Volkovs., Tomi, Poutanen., Rahul, G., Krishnan. (2023). DuETT: Dual Event Time Transformer for Electronic Health Records. arXiv.org, abs/2304.13017 doi: 10.48550/arXiv.2304.13017\n",
        "\n",
        "2. Goldberger, A., Amaral, L., Glass, L., Hausdorff, J., Ivanov, P. C., Mark, R., ... & Stanley, H. E. (2000). PhysioBank, PhysioToolkit, and PhysioNet: Components of a new research resource for complex physiologic signals. Circulation [Online]. 101 (23), pp. e215–e220.\n",
        "\n",
        "3. Ikaro Silva, George Moody, Roger Mark, and Leo Anthony Celi. Predicting mortality of\n",
        "ICU patients: The PhysioNet/Computing in Cardiology challenge 2012, Jan 2012. URL\n",
        "https://physionet.org/content/challenge-2012/1.0.0/.\n",
        "\n",
        "4. Johnson, A., Bulgarelli, L., Pollard, T., Horng, S., Celi, L. A., & Mark, R. (2022). MIMIC-IV (version 2.0). PhysioNet. https://doi.org/10.13026/7vcr-e114."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Appendix: Ablation Analysis\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Results from running model on \"sparsified\" training data (removing ~20% of observations [technically time \"bins\"] for each patient).\n",
        "\n",
        "![alt text](image-4.png)\n",
        "\n",
        "Sample illustration of removing ~20% of the data (each patient has 215 potential time bins of 45 observation parameters):\n",
        "\n",
        "![alt text](image-5.png)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
